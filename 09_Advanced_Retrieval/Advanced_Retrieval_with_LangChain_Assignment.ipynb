{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-IqJAMkwnCF"
      },
      "source": [
        "# Advanced Retrieval with LangChain\n",
        "\n",
        "In the following notebook, we'll explore various methods of advanced retrieval using LangChain!\n",
        "\n",
        "We'll touch on:\n",
        "\n",
        "- Naive Retrieval\n",
        "- Best-Matching 25 (BM25)\n",
        "- Multi-Query Retrieval\n",
        "- Parent-Document Retrieval\n",
        "- Contextual Compression (a.k.a. Rerank)\n",
        "- Ensemble Retrieval\n",
        "- Semantic chunking\n",
        "\n",
        "We'll also discuss how these methods impact performance on our set of documents with a simple RAG chain.\n",
        "\n",
        "There will be two breakout rooms:\n",
        "\n",
        "- ü§ù Breakout Room Part #1\n",
        "  - Task 1: Getting Dependencies!\n",
        "  - Task 2: Data Collection and Preparation\n",
        "  - Task 3: Setting Up QDrant!\n",
        "  - Task 4-10: Retrieval Strategies\n",
        "- ü§ù Breakout Room Part #2\n",
        "  - Activity: Evaluate with Ragas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rKP3hgHivpe"
      },
      "source": [
        "# ü§ù Breakout Room Part #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xes8oT-xHN7"
      },
      "source": [
        "## Task 1: Getting Dependencies!\n",
        "\n",
        "We're going to need a few specific LangChain community packages, like OpenAI (for our [LLM](https://platform.openai.com/docs/models) and [Embedding Model](https://platform.openai.com/docs/guides/embeddings)) and Cohere (for our [Reranker](https://cohere.com/rerank))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7OHJXzfyJyA"
      },
      "source": [
        "We'll also provide our OpenAI key, as well as our Cohere API key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LttlDQUYgSI",
        "outputId": "9dca95ab-4d02-4adf-ec3f-cb831326dc54"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3iUahNiJyQbv",
        "outputId": "78bf06ef-2ee8-46c3-f73d-27958b4dd79b"
      },
      "outputs": [],
      "source": [
        "os.environ[\"COHERE_API_KEY\"] = getpass.getpass(\"Cohere API Key:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw304iAFyRtl"
      },
      "source": [
        "## Task 2: Data Collection and Preparation\n",
        "\n",
        "We'll be using our Use Case Data once again - this time the strutured data available through the CSV!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A92NC2QZzCsi"
      },
      "source": [
        "### Data Preparation\n",
        "\n",
        "We want to make sure all our documents have the relevant metadata for the various retrieval strategies we're going to be applying today."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GshBjVRJZ6p8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "loader = CSVLoader(\n",
        "    file_path=f\"./data/Projects_with_Domains.csv\",\n",
        "    metadata_columns=[\n",
        "      \"Project Title\",\n",
        "      \"Project Domain\",\n",
        "      \"Secondary Domain\",\n",
        "      \"Description\",\n",
        "      \"Judge Comments\",\n",
        "      \"Score\",\n",
        "      \"Project Name\",\n",
        "      \"Judge Score\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "synthetic_usecase_data = loader.load()\n",
        "\n",
        "for doc in synthetic_usecase_data:\n",
        "    doc.page_content = doc.metadata[\"Description\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gQphb6y0C0S"
      },
      "source": [
        "Let's look at an example document to see if everything worked as expected!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkUkCf7DaMiq",
        "outputId": "e90bd5da-1d87-423b-838a-cb6efc16b199"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': './data/Projects_with_Domains.csv', 'row': 0, 'Project Title': 'InsightAI 1', 'Project Domain': 'Security', 'Secondary Domain': 'Finance / FinTech', 'Description': 'A low-latency inference system for multimodal agents in autonomous systems.', 'Judge Comments': 'Technically ambitious and well-executed.', 'Score': '85', 'Project Name': 'Project Aurora', 'Judge Score': '9.5'}, page_content='A low-latency inference system for multimodal agents in autonomous systems.')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "synthetic_usecase_data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWaQpdHl0Gzc"
      },
      "source": [
        "## Task 3: Setting up QDrant!\n",
        "\n",
        "Now that we have our documents, let's create a QDrant VectorStore with the collection name \"Synthetic_Usecases\".\n",
        "\n",
        "We'll leverage OpenAI's [`text-embedding-3-small`](https://openai.com/blog/new-embedding-models-and-api-updates) because it's a very powerful (and low-cost) embedding model.\n",
        "\n",
        "> NOTE: We'll be creating additional vectorstores where necessary, but this pattern is still extremely useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NT8ihRJbYmMT"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "vectorstore = Qdrant.from_documents(\n",
        "    synthetic_usecase_data,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecases\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x2SS4Rh0hiN"
      },
      "source": [
        "## Task 4: Naive RAG Chain\n",
        "\n",
        "Since we're focusing on the \"R\" in RAG today - we'll create our Retriever first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEH7X5Ai08FH"
      },
      "source": [
        "### R - Retrieval\n",
        "\n",
        "This naive retriever will simply look at each review as a document, and use cosine-similarity to fetch the 10 most relevant documents.\n",
        "\n",
        "> NOTE: We're choosing `10` as our `k` here to provide enough documents for our reranking process later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "GFDPrNBtb72o"
      },
      "outputs": [],
      "source": [
        "naive_retriever = vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbBhyQjz06dx"
      },
      "source": [
        "### A - Augmented\n",
        "\n",
        "We're going to go with a standard prompt for our simple RAG chain today! Nothing fancy here, we want this to mostly be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7uSz-Dbqcoki"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_TEMPLATE = \"\"\"\\\n",
        "You are a helpful and kind assistant. Use the context provided below to answer the question.\n",
        "\n",
        "If you do not know the answer, or are unsure, say you don't know.\n",
        "\n",
        "Query:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlRzpb231GGJ"
      },
      "source": [
        "### G - Generation\n",
        "\n",
        "We're going to leverage `gpt-4.1-nano` as our LLM today, as - again - we want this to largely be about the Retrieval process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "c-1t9H60dJLg"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "chat_model = ChatOpenAI(model=\"gpt-4.1-nano\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg3QRGzA1M2x"
      },
      "source": [
        "### LCEL RAG Chain\n",
        "\n",
        "We're going to use LCEL to construct our chain.\n",
        "\n",
        "> NOTE: This chain will be exactly the same across the various examples with the exception of our Retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0bvstS7mdOW3"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from operator import itemgetter\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "naive_retrieval_chain = (\n",
        "    # INVOKE CHAIN WITH: {\"question\" : \"<<SOME USER QUESTION>>\"}\n",
        "    # \"question\" : populated by getting the value of the \"question\" key\n",
        "    # \"context\"  : populated by getting the value of the \"question\" key and chaining it into the base_retriever\n",
        "    {\"context\": itemgetter(\"question\") | naive_retriever, \"question\": itemgetter(\"question\")}\n",
        "    # \"context\"  : is assigned to a RunnablePassthrough object (will not be called or considered in the next step)\n",
        "    #              by getting the value of the \"context\" key from the previous step\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    # \"response\" : the \"context\" and \"question\" values are used to format our prompt object and then piped\n",
        "    #              into the LLM and stored in a key called \"response\"\n",
        "    # \"context\"  : populated by getting the value of the \"context\" key from the previous step\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izKujhNb1ZG8"
      },
      "source": [
        "Let's see how this simple chain does on a few different prompts.\n",
        "\n",
        "> NOTE: You might think that we've cherry picked prompts that showcase the individual skill of each of the retrieval strategies - you'd be correct!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "LI-5ueEddku9",
        "outputId": "7f3cec18-5f4e-41bb-cf71-51ba0be5388e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Healthcare / MedTech,\" as it is mentioned multiple times among the projects.'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "43zdcdUydtXh",
        "outputId": "db874e67-f568-4ed1-b863-b7c17b387052"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security. Specifically, one project, \"LatticeFlow,\" is described as an AI model compression suite enabling on-device reasoning for IoT sensors, which falls under the secondary domain of Security. Additionally, another project, \"Pathfinder 24,\" focuses on an AI-powered platform optimizing logistics routes for sustainability, which is linked to security in the context of healthcare / MedTech.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lpG6rlvvvKFq",
        "outputId": "a1b330b0-628e-41be-d829-9c1d55e781f5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had a generally positive view of the fintech projects, often highlighting their technical strength and real-world impact. For example, one judge described a fintech project as a \"clever solution with measurable environmental benefit,\" and another called a project \"impressive\" with \"robust experimental validation.\" Overall, the judges appreciated the quality, ambition, and potential impact of the fintech-related projects, though some noted minor issues such as integration challenges or the need for further benchmarking.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "naive_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsbfQmbr1leg"
      },
      "source": [
        "Overall, this is not bad! Let's see if we can make it better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1vt8HPR16w"
      },
      "source": [
        "## Task 5: Best-Matching 25 (BM25) Retriever\n",
        "\n",
        "Taking a step back in time - [BM25](https://www.nowpublishers.com/article/Details/INR-019) is based on [Bag-Of-Words](https://en.wikipedia.org/wiki/Bag-of-words_model) which is a sparse representation of text.\n",
        "\n",
        "In essence, it's a way to compare how similar two pieces of text are based on the words they both contain.\n",
        "\n",
        "This retriever is very straightforward to set-up! Let's see it happen down below!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qdF4wuj5R-cG"
      },
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(synthetic_usecase_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIjJlBQ8drKH"
      },
      "source": [
        "We'll construct the same chain - only changing the retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WR15EQG7SLuw"
      },
      "outputs": [],
      "source": [
        "bm25_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | bm25_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Gi-yXCDdvJk"
      },
      "source": [
        "Let's look at the responses!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "oY9qzmm3SOrF",
        "outputId": "4d4f450f-5978-460f-f242-b32407868353"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the project domains mentioned include Productivity Assistants, E-commerce / Marketplaces, Healthcare / MedTech, and Finance / FinTech. Since the dataset snippet shows multiple projects with various domains but does not specify the total counts, I cannot definitively determine the most common domain. \\n\\nHowever, among the listed projects, Finance / FinTech appears twice (PulseAI 50 and DocuCheck 47), which suggests it might be a common domain in this sample. \\n\\nIf you need a precise answer based on the full data, I recommend analyzing the entire dataset to identify the domain with the highest frequency.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "igfinyneSQkh",
        "outputId": "9752d4a9-dd16-45b1-f63f-a76e93a05eb3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there was a use case related to security. The project titled \"SecureNest 49\" in the domain of E-commerce / Marketplaces with a secondary focus on Legal / Compliance involved a document summarization and retrieval system for enterprise knowledge bases, which relates to security and compliance aspects.'"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w0H7pV_USSMQ",
        "outputId": "bdead654-3109-4143-9a30-e1d6ca8dc534"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had the following comments about the fintech projects:\\n\\n- For the project \"SynthMind\" in the finance/fintech domain, the judges noted that it had a conceptually strong approach but indicated that the results need more benchmarking.\\n- For the project \"PulseAI 50,\" which is also in the finance/fintech secondary domain, the judges described it as \"Technically ambitious and well-executed.\"'"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "bm25_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvg5xHaUdxCl"
      },
      "source": [
        "It's not clear that this is better or worse, if only we had a way to test this (SPOILERS: We do, the second half of the notebook will cover this)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "Give an example query where BM25 is better than embeddings and justify your answer.\n",
        "\n",
        "##### ‚úÖ Answer\n",
        "\n",
        "BM25 will be better in case that there is a file with specific terminology or name that we want to search about. For example in a file with space and planets. Assume that in the file there is a sentence like: `The Kepler-186f exoplanet is located about 500 light-years away and may have conditions similar to Earth.`<br>\n",
        "If these are chunked by characters  and the query is `What is known about Kepler-186f?`, the BM25 finds it immediately because contains exact tokens \"Kepler-186f\".<br>BM25 wins ‚Äî because character-level embeddings destroy semantic integrity, while BM25 still recognizes exact term overlap (Kepler-186f)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-dcbFn2vpZF"
      },
      "source": [
        "## Task 6: Contextual Compression (Using Reranking)\n",
        "\n",
        "Contextual Compression is a fairly straightforward idea: We want to \"compress\" our retrieved context into just the most useful bits.\n",
        "\n",
        "There are a few ways we can achieve this - but we're going to look at a specific example called reranking.\n",
        "\n",
        "The basic idea here is this:\n",
        "\n",
        "- We retrieve lots of documents that are very likely related to our query vector\n",
        "- We \"compress\" those documents into a smaller set of *more* related documents using a reranking algorithm.\n",
        "\n",
        "We'll be leveraging Cohere's Rerank model for our reranker today!\n",
        "\n",
        "All we need to do is the following:\n",
        "\n",
        "- Create a basic retriever\n",
        "- Create a compressor (reranker, in this case)\n",
        "\n",
        "That's it!\n",
        "\n",
        "Let's see it in the code below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "psHvO2K1v_ZQ"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=compressor, base_retriever=naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TA9RB2x-j7P"
      },
      "source": [
        "Let's create our chain again, and see how this does!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "1BXqmxvHwX6T"
      },
      "outputs": [],
      "source": [
        "contextual_compression_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | compression_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V3iGpokswcBb",
        "outputId": "f15d2aa1-5e8b-417d-f623-eb835d072e59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, the most common project domain appears to be \"Security,\" as it is listed for at least one project in the sample. However, since the sample includes only a few projects, and the data source is a CSV file, I cannot definitively determine the most common domain without analyzing the entire dataset.\\n\\nIf you have the complete dataset or additional information, I can help analyze it further.'"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "7u_k0i4OweUd",
        "outputId": "be5fccc8-2352-4189-c524-bbeaa28cf799"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific use cases explicitly related to security. The use cases mentioned mainly focus on privacy improvements in healthcare applications through federated learning.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "zn1EqaGqweXN",
        "outputId": "42bc5972-4164-46eb-f49d-4272f39bb89b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had positive comments about the fintech projects. For example, one project, Pathfinder 27, was praised for its excellent code quality and use of open-source libraries, receiving a high judge score of 9.8. Additionally, a project called PlanPilot 35 was described as a clever solution with measurable environmental benefits and received a judge score of 8.4. Overall, the judges appreciated the quality, innovation, and potential impact of the fintech projects.'"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "contextual_compression_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEbT0g2S-mZ4"
      },
      "source": [
        "We'll need to rely on something like Ragas to help us get a better sense of how this is performing overall - but it \"feels\" better!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqbghrBEQNn5"
      },
      "source": [
        "## Task 7: Multi-Query Retriever\n",
        "\n",
        "Typically in RAG we have a single query - the one provided by the user.\n",
        "\n",
        "What if we had....more than one query!\n",
        "\n",
        "In essence, a Multi-Query Retriever works by:\n",
        "\n",
        "1. Taking the original user query and creating `n` number of new user queries using an LLM.\n",
        "2. Retrieving documents for each query.\n",
        "3. Using all unique retrieved documents as context\n",
        "\n",
        "So, how is it to set-up? Not bad! Let's see it down below!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "pfM26ReXQjzU"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=naive_retriever, llm=chat_model\n",
        ") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "1vRc129jQ5WW"
      },
      "outputs": [],
      "source": [
        "multi_query_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | multi_query_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "CGgNuOb3Q3M9",
        "outputId": "c5273ecf-da35-40b8-fbdb-0f8beab425f7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided data, there are multiple project domains such as Healthcare / MedTech, Developer Tools / DevEx, E-commerce / Marketplaces, Legal / Compliance, Finance / FinTech, Data / Analytics, Sales / CRM, Customer Support / Helpdesk, Productivity Assistants, Security, Creative / Design / Media, and Writing & Content.\\n\\nAmong these, the most frequently mentioned project domain appears to be **Writing & Content**. Several projects are categorized under this domain, indicating it is the most common project domain in the dataset.'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aAlSthxrRDBC",
        "outputId": "230ff807-23ae-4d25-8d11-cfdbed0b77cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are usecases related to security. One example is the project titled \"SecureNest,\" which focuses on a hardware-aware model quantization benchmark suite with a strong concept in the security and compliance domain.'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "Uv1mpCK8REs4",
        "outputId": "00fbc22a-ed9b-4613-9695-0b179e3f8369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had generally positive comments about the fintech projects. For example, one judge described the project as a \"clever solution with measurable environmental benefit,\" indicating appreciation for innovation and impact. Another commented that the project was \"technically ambitious and well-executed,\" highlighting its strong technical foundation. Additionally, some projects received praise for their promising ideas with robust validation, or for their potential real-world impact. Overall, the judges recognized the fintech projects for their ingenuity, technical rigor, and potential benefits.'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "multi_query_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Explain how generating multiple reformulations of a user query can improve recall.\n",
        "\n",
        "##### ‚úÖ Answer\n",
        "A single user query is often too narrow or ambiguous. For example in previous example with query:`planets similar to Earth`, if the documents describe:‚ÄúEarth-like exoplanets discovered by Kepler‚Äù,‚ÄúHabitable zone worlds around distant stars‚Äù,‚ÄúTerrestrial planets with liquid water potential‚Äù, a simple lexical or embedding-based search might miss some because the exact wording ‚Äúplanets similar to Earth‚Äù doesn‚Äôt appear in every text. A simple lexical or embedding-based search might miss some because the exact wording ‚Äúplanets similar to Earth‚Äù doesn‚Äôt appear in every text. \n",
        "\n",
        "Solution:Generating multiple reformulations of a query increases recall because it helps the retriever find different relevant documents that express the same concept using different language or terminology."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDEawBf_d_3G"
      },
      "source": [
        "## Task 8: Parent Document Retriever\n",
        "\n",
        "A \"small-to-big\" strategy - the Parent Document Retriever works based on a simple strategy:\n",
        "\n",
        "1. Each un-split \"document\" will be designated as a \"parent document\" (You could use larger chunks of document as well, but our data format allows us to consider the overall document as the parent chunk)\n",
        "2. Store those \"parent documents\" in a memory store (not a VectorStore)\n",
        "3. We will chunk each of those documents into smaller documents, and associate them with their respective parents, and store those in a VectorStore. We'll call those \"child chunks\".\n",
        "4. When we query our Retriever, we will do a similarity search comparing our query vector to the \"child chunks\".\n",
        "5. Instead of returning the \"child chunks\", we'll return their associated \"parent chunks\".\n",
        "\n",
        "Okay, maybe that was a few steps - but the basic idea is this:\n",
        "\n",
        "- Search for small documents\n",
        "- Return big documents\n",
        "\n",
        "The intuition is that we're likely to find the most relevant information by limiting the amount of semantic information that is encoded in each embedding vector - but we're likely to miss relevant surrounding context if we only use that information.\n",
        "\n",
        "Let's start by creating our \"parent documents\" and defining a `RecursiveCharacterTextSplitter`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "qJ53JJuMd_ZH"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "parent_docs = synthetic_usecase_data\n",
        "child_splitter = RecursiveCharacterTextSplitter(chunk_size=750)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOpXfVUH3gL3"
      },
      "source": [
        "We'll need to set up a new QDrant vectorstore - and we'll use another useful pattern to do so!\n",
        "\n",
        "> NOTE: We are manually defining our embedding dimension, you'll need to change this if you're using a different embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzFc-_9HlGQ-",
        "outputId": "223662dd-c36f-42f7-d1b0-b086e571484e"
      },
      "outputs": [],
      "source": [
        "from langchain_qdrant import QdrantVectorStore\n",
        "\n",
        "client = QdrantClient(location=\":memory:\")\n",
        "\n",
        "client.create_collection(\n",
        "    collection_name=\"full_documents\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "parent_document_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"full_documents\", embedding=OpenAIEmbeddings(model=\"text-embedding-3-small\"), client=client\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf_g95FA3s6w"
      },
      "source": [
        "Now we can create our `InMemoryStore` that will hold our \"parent documents\" - and build our retriever!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BpWVjPf4fLUp"
      },
      "outputs": [],
      "source": [
        "store = InMemoryStore()\n",
        "\n",
        "parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore = parent_document_vectorstore,\n",
        "    docstore=store,\n",
        "    child_splitter=child_splitter,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoYmSWfE32Zo"
      },
      "source": [
        "By default, this is empty as we haven't added any documents - let's add some now!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iQ2ZzfKigMZc"
      },
      "outputs": [],
      "source": [
        "parent_document_retriever.add_documents(parent_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bI7Tip1335rE"
      },
      "source": [
        "We'll create the same chain we did before - but substitute our new `parent_document_retriever`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "Qq_adt2KlSqp"
      },
      "outputs": [],
      "source": [
        "parent_document_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | parent_document_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNolUVQb4Apt"
      },
      "source": [
        "Let's give it a whirl!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "TXB5i89Zly5W",
        "outputId": "94c240be-7c5b-4c58-9eee-56d93285a054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"Healthcare / MedTech,\" as it is mentioned in multiple project entries.'"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "V5F1T-wNl3cg",
        "outputId": "9b81e72e-5db7-4b8a-b25b-400ea0df5335"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Based on the provided context, there are no specific use cases about security explicitly mentioned. The projects focus on federated learning and improving privacy in healthcare applications, which relates to security and privacy concerns, but there are no direct references to security use cases.'"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ZqARszGzvGcG",
        "outputId": "8867f83c-db13-4db4-d57f-9bd51d32cd8a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The judges had positive comments about the fintech projects. For example, one project was described as \"A clever solution with measurable environmental benefit,\" indicating that the judges appreciated its innovation and impact. Another project was noted as \"Technically ambitious and well-executed,\" suggesting a high level of technical quality and execution. Overall, the judges recognized the projects for their creativity, impact, and soundness.'"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parent_document_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B41cj42s4DPM"
      },
      "source": [
        "Overall, the performance *seems* largely the same. We can leverage a tool like [Ragas]() to more effectively answer the question about the performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUrIBKl_TwS9"
      },
      "source": [
        "## Task 9: Ensemble Retriever\n",
        "\n",
        "In brief, an Ensemble Retriever simply takes 2, or more, retrievers and combines their retrieved documents based on a rank-fusion algorithm.\n",
        "\n",
        "In this case - we're using the [Reciprocal Rank Fusion](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf) algorithm.\n",
        "\n",
        "Setting it up is as easy as providing a list of our desired retrievers - and the weights for each retriever."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "8j7jpZsKTxic"
      },
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [bm25_retriever, naive_retriever, parent_document_retriever, compression_retriever, multi_query_retriever]\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=retriever_list, weights=equal_weighting\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpo9Psl5hhJ-"
      },
      "source": [
        "We'll pack *all* of these retrievers together in an ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "KZ__EZwpUKkd"
      },
      "outputs": [],
      "source": [
        "ensemble_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | ensemble_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSsvHpRMj24L"
      },
      "source": [
        "Let's look at our results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lMvqL88UQI-",
        "outputId": "d86dd5f7-0a13-4836-c0ce-cc4c431fd889"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data appears to be \"E‚Äëcommerce / Marketplaces,\" as it is mentioned more than once among the sample projects.'"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "MNFWLYECURI1",
        "outputId": "b17973b5-66a9-4481-97d5-880b5754b5c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there was at least one use case related to security. The project titled \"SecureNest\" involves a document summarization and retrieval system for enterprise knowledge bases, which is associated with the domain of E-commerce / Marketplaces and secondary domain of Legal / Compliance. The description indicates it is a comprehensive and technically mature approach, likely addressing security in the context of managing sensitive enterprise information.'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "A7qbHfWgUR4c",
        "outputId": "f7373144-59ef-4fc7-b75d-ca00e7df881e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had various comments about the fintech projects. For the project \"SynthMind,\" judges described it as having a strong conceptual foundation but noted that the results require more benchmarking. Overall, the feedback indicates that while some fintech projects are conceptually solid, there is a need for more comprehensive evaluation to fully demonstrate their effectiveness.'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ensemble_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MopbkNJAXVaN"
      },
      "source": [
        "## Task 10: Semantic Chunking\n",
        "\n",
        "While this is not a retrieval method - it *is* an effective way of increasing retrieval performance on corpora that have clean semantic breaks in them.\n",
        "\n",
        "Essentially, Semantic Chunking is implemented by:\n",
        "\n",
        "1. Embedding all sentences in the corpus.\n",
        "2. Combining or splitting sequences of sentences based on their semantic similarity based on a number of [possible thresholding methods](https://python.langchain.com/docs/how_to/semantic-chunker/):\n",
        "  - `percentile`\n",
        "  - `standard_deviation`\n",
        "  - `interquartile`\n",
        "  - `gradient`\n",
        "3. Each sequence of related sentences is kept as a document!\n",
        "\n",
        "Let's see how to implement this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9ciZbFEldv_"
      },
      "source": [
        "We'll use the `percentile` thresholding method for this example which will:\n",
        "\n",
        "Calculate all distances between sentences, and then break apart sequences of setences that exceed a given percentile among all distances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "66EIEWiEYl5y"
      },
      "outputs": [],
      "source": [
        "from langchain_experimental.text_splitter import SemanticChunker\n",
        "\n",
        "semantic_chunker = SemanticChunker(\n",
        "    embeddings,\n",
        "    breakpoint_threshold_type=\"percentile\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqoKmz12mhRW"
      },
      "source": [
        "Now we can split our documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ROcV7o68ZIq7"
      },
      "outputs": [],
      "source": [
        "semantic_documents = semantic_chunker.split_documents(synthetic_usecase_data[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8-LNC-Xmjex"
      },
      "source": [
        "Let's create a new vector store."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "h3sl9QjyZhIe"
      },
      "outputs": [],
      "source": [
        "semantic_vectorstore = Qdrant.from_documents(\n",
        "    semantic_documents,\n",
        "    embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"Synthetic_Usecase_Data_Semantic_Chunks\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eh_r_-LHmmKn"
      },
      "source": [
        "We'll use naive retrieval for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "odVyDUHwZftc"
      },
      "outputs": [],
      "source": [
        "semantic_retriever = semantic_vectorstore.as_retriever(search_kwargs={\"k\" : 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkeiv_ojmp6G"
      },
      "source": [
        "Finally we can create our classic chain!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "xWE_0J0mZveG"
      },
      "outputs": [],
      "source": [
        "semantic_retrieval_chain = (\n",
        "    {\"context\": itemgetter(\"question\") | semantic_retriever, \"question\": itemgetter(\"question\")}\n",
        "    | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "    | {\"response\": rag_prompt | chat_model, \"context\": itemgetter(\"context\")}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5pfjLQ3ms9_"
      },
      "source": [
        "And view the results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "0lN2j-e4Z0SD",
        "outputId": "ef483e21-7200-4dfc-b8bf-aed4f23587b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The most common project domain in the provided data is \"Legal / Compliance,\" which appears twice among the listed projects.'"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What is the most common project domain?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "xdqfBH1SZ3f9",
        "outputId": "ed62b2d1-7586-46cc-aaf4-c54192a56155"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Yes, there are use cases related to security. Specifically, projects such as \"MediMind 17\" and \"AutoMate 11\" are in the security domain. \"MediMind 17\" involves a medical imaging solution, and \"AutoMate 11\" features a reinforcement learning setup for optimizing energy efficiency in data centers, which can be linked to security aspects of infrastructure.'"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"Were there any usecases about security?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "rAcAObZnZ4o6",
        "outputId": "3f1cade3-41e4-4e42-ef71-048dd18e5e3a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Judges had a generally positive view of the fintech projects. For example, the project \"TrendLens 19\" was described as \"Technically ambitious and well-executed,\" and \"AutoMate 5\" was noted for being \"A forward-looking idea with solid supporting data.\" Overall, the judges appreciated the technical quality and potential of the fintech-related projects.'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "semantic_retrieval_chain.invoke({\"question\" : \"What did judges have to say about the fintech projects?\"})[\"response\"].content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "If sentences are short and highly repetitive (e.g., FAQs), how might semantic chunking behave, and how would you adjust the algorithm?\n",
        "\n",
        "##### ‚úÖ Answer\n",
        "Can use keyword and semantic search together: BM25 finds exact word matches,Embeddings find similar meanings.<br> Rerank with a cross-encoder can be used.<br> Use a ParentDocumentRetriever to keep answers intact.<br>Use no overlap and short chunk size only if the atomic FAQ fits (typically 100‚Äì300 tokens).<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk2n3-pnVWDJ"
      },
      "source": [
        "# ü§ù Breakout Room Part #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SkJLYwMVZkj"
      },
      "source": [
        "#### üèóÔ∏è Activity #1\n",
        "\n",
        "Your task is to evaluate the various Retriever methods against eachother.\n",
        "\n",
        "You are expected to:\n",
        "\n",
        "1. Create a \"golden dataset\"\n",
        " - Use Synthetic Data Generation (powered by Ragas, or otherwise) to create this dataset\n",
        "2. Evaluate each retriever with *retriever specific* Ragas metrics\n",
        " - Semantic Chunking is not considered a retriever method and will not be required for marks, but you may find it useful to do a \"semantic chunking on\" vs. \"semantic chunking off\" comparision between them\n",
        "3. Compile these in a list and write a small paragraph about which is best for this particular data and why.\n",
        "\n",
        "Your analysis should factor in:\n",
        "  - Cost\n",
        "  - Latency\n",
        "  - Performance\n",
        "\n",
        "> NOTE: This is **NOT** required to be completed in class. Please spend time in your breakout rooms creating a plan before moving on to writing code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWAr16a5XMub"
      },
      "source": [
        "##### HINTS:\n",
        "\n",
        "- LangSmith provides detailed information about latency and cost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "\n",
        "loader = PyMuPDFLoader(\"data/howpeopleuseai.pdf\")\n",
        "pdf_docs = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nickm\\AppData\\Local\\Temp\\ipykernel_24964\\3377917698.py:6: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use the modern LLM providers instead: from ragas.llms.base import llm_factory; llm = llm_factory('gpt-4o-mini') or from ragas.llms.base import instructor_llm_factory; llm = instructor_llm_factory('openai', client=openai_client)\n",
            "  generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\"))\n",
            "C:\\Users\\nickm\\AppData\\Local\\Temp\\ipykernel_24964\\3377917698.py:7: DeprecationWarning: LangchainEmbeddingsWrapper is deprecated and will be removed in a future version. Use the modern embedding providers instead: embedding_factory('openai', model='text-embedding-3-small', client=openai_client) or from ragas.embeddings import OpenAIEmbeddings, GoogleEmbeddings, HuggingFaceEmbeddings\n",
            "  generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.embeddings import LangchainEmbeddingsWrapper\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "generator_llm = LangchainLLMWrapper(ChatOpenAI(model=\"gpt-4o\")) \n",
        "generator_embeddings = LangchainEmbeddingsWrapper(OpenAIEmbeddings())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "67d370168694498692266e7901a5c722",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlinesExtractor:   0%|          | 0/21 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aff9269fbf2c4ad2a0a5823d9b90553a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying HeadlineSplitter:   0%|          | 0/64 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dcdf26d351374b86b4f8821aae3e8808",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying SummaryExtractor:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary' already exists in node '6dccc4'. Skipping!\n",
            "Property 'summary' already exists in node '7a3782'. Skipping!\n",
            "Property 'summary' already exists in node '1cbf38'. Skipping!\n",
            "Property 'summary' already exists in node '1cc502'. Skipping!\n",
            "Property 'summary' already exists in node 'c80d6e'. Skipping!\n",
            "Property 'summary' already exists in node '09f3f1'. Skipping!\n",
            "Property 'summary' already exists in node 'e719a7'. Skipping!\n",
            "Property 'summary' already exists in node 'd3050a'. Skipping!\n",
            "Property 'summary' already exists in node '1d3eda'. Skipping!\n",
            "Property 'summary' already exists in node '22d286'. Skipping!\n",
            "Property 'summary' already exists in node '75fcba'. Skipping!\n",
            "Property 'summary' already exists in node '86ef5d'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d8f6f227ed4e4710a903b759293a232d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CustomNodeFilter:   0%|          | 0/19 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ffd946bb35874e09ba635d1807073f74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying EmbeddingExtractor:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Property 'summary_embedding' already exists in node 'd3050a'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '6dccc4'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '7a3782'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '75fcba'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '22d286'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'c80d6e'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '1cc502'. Skipping!\n",
            "Property 'summary_embedding' already exists in node 'e719a7'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '86ef5d'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '1d3eda'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '1cbf38'. Skipping!\n",
            "Property 'summary_embedding' already exists in node '09f3f1'. Skipping!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "277c792f0e6140bc9fb3d8020b03953a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying ThemesExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "950ca288b5f94804810c1b0e5f5b1184",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying NERExtractor:   0%|          | 0/14 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c1090706e7274536b7cdd1f87f8300ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying CosineSimilarityBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6866965312dd4cb3bf82bb63a336df70",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying OverlapScoreBuilder:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f96b8dabd89d41988eeb0104c83dc409",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating personas:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "183f9f1240924f2b8c0ddc0b5e10a228",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Scenarios:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d25ab8cf9c1e4dee98d761cc432e311c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating Samples:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from ragas.testset import TestsetGenerator\n",
        "from ragas.testset.synthesizers import  SingleHopSpecificQuerySynthesizer, MultiHopAbstractQuerySynthesizer, MultiHopSpecificQuerySynthesizer\n",
        "\n",
        "\n",
        "generator = TestsetGenerator(\n",
        "    llm=generator_llm, \n",
        "    embedding_model=generator_embeddings,\n",
        ")\n",
        "\n",
        "dataset = generator.generate_with_langchain_docs(\n",
        "    pdf_docs, \n",
        "    testset_size=10,\n",
        "    query_distribution=[\n",
        "        (SingleHopSpecificQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "        (MultiHopAbstractQuerySynthesizer(llm=generator_llm), 0.5),\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate Naive Retriever with Ragas Metrics\n",
        "\n",
        "Now let's use the synthetic dataset to evaluate our naive retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>reference_contexts</th>\n",
              "      <th>reference</th>\n",
              "      <th>synthesizer_name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wht did Bick et al., 2024 say about the speed ...</td>\n",
              "      <td>[Introduction\\nChatGPT launched in November 20...</td>\n",
              "      <td>Bick et al., 2024 noted that the speed of glob...</td>\n",
              "      <td>single_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Who is Brynjolfsson?</td>\n",
              "      <td>[Month\\nNon-Work (M)\\n(%)\\nWork (M)\\n(%)\\nTota...</td>\n",
              "      <td>Brynjolfsson is mentioned in the context as pa...</td>\n",
              "      <td>single_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What does Handa et al. (2025) report about the...</td>\n",
              "      <td>[Two of our findings stand in contrast to othe...</td>\n",
              "      <td>Handa et al. (2025) report that 37% of convers...</td>\n",
              "      <td>single_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What Caplin et al. say about ChatGPT and how i...</td>\n",
              "      <td>[Doing, and that Asking messages are consisten...</td>\n",
              "      <td>Caplin et al. (2023) argue that ChatGPT likely...</td>\n",
              "      <td>single_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How do large language models (LLMs) function a...</td>\n",
              "      <td>[What is ChatGPT?\\nHere we give a simplified o...</td>\n",
              "      <td>An LLM can be thought of as a function from a ...</td>\n",
              "      <td>single_hop_specific_query_synthesizer</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  Wht did Bick et al., 2024 say about the speed ...   \n",
              "1                               Who is Brynjolfsson?   \n",
              "2  What does Handa et al. (2025) report about the...   \n",
              "3  What Caplin et al. say about ChatGPT and how i...   \n",
              "4  How do large language models (LLMs) function a...   \n",
              "\n",
              "                                  reference_contexts  \\\n",
              "0  [Introduction\\nChatGPT launched in November 20...   \n",
              "1  [Month\\nNon-Work (M)\\n(%)\\nWork (M)\\n(%)\\nTota...   \n",
              "2  [Two of our findings stand in contrast to othe...   \n",
              "3  [Doing, and that Asking messages are consisten...   \n",
              "4  [What is ChatGPT?\\nHere we give a simplified o...   \n",
              "\n",
              "                                           reference  \\\n",
              "0  Bick et al., 2024 noted that the speed of glob...   \n",
              "1  Brynjolfsson is mentioned in the context as pa...   \n",
              "2  Handa et al. (2025) report that 37% of convers...   \n",
              "3  Caplin et al. (2023) argue that ChatGPT likely...   \n",
              "4  An LLM can be thought of as a function from a ...   \n",
              "\n",
              "                        synthesizer_name  \n",
              "0  single_hop_specific_query_synthesizer  \n",
              "1  single_hop_specific_query_synthesizer  \n",
              "2  single_hop_specific_query_synthesizer  \n",
              "3  single_hop_specific_query_synthesizer  \n",
              "4  single_hop_specific_query_synthesizer  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert to pandas to see the data more clearly\n",
        "import pandas as pd\n",
        "df_dataset = dataset.to_pandas()\n",
        "df_dataset.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Vector Stores from PDF Documents\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naive Retiever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Qdrant\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "pdf_embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "pdf_vectorstore = Qdrant.from_documents(\n",
        "    pdf_docs,\n",
        "    pdf_embeddings,\n",
        "    location=\":memory:\",\n",
        "    collection_name=\"PDF_Documents\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "pdf_naive_retriever = pdf_vectorstore.as_retriever(search_kwargs={\"k\": 10})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "BM 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.retrievers import BM25Retriever\n",
        "\n",
        "pdf_bm25_retriever = BM25Retriever.from_documents(pdf_docs)\n",
        "pdf_bm25_retriever.k = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "MultiQueryRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
        "\n",
        "pdf_multi_query_retriever = MultiQueryRetriever.from_llm(\n",
        "    retriever=pdf_naive_retriever,\n",
        "    llm=chat_model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ParentDocumentRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers import ParentDocumentRetriever\n",
        "from langchain.storage import InMemoryStore\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from qdrant_client import QdrantClient, models\n",
        "\n",
        "pdf_child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
        "\n",
        "pdf_parent_client = QdrantClient(location=\":memory:\")\n",
        "pdf_parent_client.create_collection(\n",
        "    collection_name=\"pdf_parent_docs\",\n",
        "    vectors_config=models.VectorParams(size=1536, distance=models.Distance.COSINE)\n",
        ")\n",
        "\n",
        "pdf_parent_vectorstore = QdrantVectorStore(\n",
        "    collection_name=\"pdf_parent_docs\",\n",
        "    embedding=pdf_embeddings,\n",
        "    client=pdf_parent_client\n",
        ")\n",
        "\n",
        "# Create parent document retriever\n",
        "pdf_parent_store = InMemoryStore()\n",
        "pdf_parent_document_retriever = ParentDocumentRetriever(\n",
        "    vectorstore=pdf_parent_vectorstore,\n",
        "    docstore=pdf_parent_store,\n",
        "    child_splitter=pdf_child_splitter,\n",
        ")\n",
        "\n",
        "pdf_parent_document_retriever.add_documents(pdf_docs, ids=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ContextualCompressionRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
        "from langchain_cohere import CohereRerank\n",
        "\n",
        "pdf_compressor = CohereRerank(model=\"rerank-v3.5\")\n",
        "pdf_compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=pdf_compressor,\n",
        "    base_retriever=pdf_naive_retriever\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "EnsembleRetriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "pdf_retriever_list = [\n",
        "    pdf_bm25_retriever,\n",
        "    pdf_naive_retriever,\n",
        "    pdf_multi_query_retriever\n",
        "]\n",
        "\n",
        "pdf_ensemble_retriever = EnsembleRetriever(\n",
        "    retrievers=pdf_retriever_list,\n",
        "    weights=[1/3, 1/3, 1/3]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nickm\\AppData\\Local\\Temp\\ipykernel_24964\\2216582654.py:43: DeprecationWarning: LangchainLLMWrapper is deprecated and will be removed in a future version. Use the modern LLM providers instead: from ragas.llms.base import llm_factory; llm = llm_factory('gpt-4o-mini') or from ragas.llms.base import instructor_llm_factory; llm = instructor_llm_factory('openai', client=openai_client)\n",
            "  evaluator_llm = LangchainLLMWrapper(chat_model)\n"
          ]
        }
      ],
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.schema import AIMessage\n",
        "from ragas import EvaluationDataset, evaluate, RunConfig\n",
        "from ragas.llms import LangchainLLMWrapper\n",
        "from ragas.metrics import (\n",
        "    LLMContextRecall, Faithfulness, FactualCorrectness,\n",
        ")\n",
        "\n",
        "def reset_eval_fields(ds):\n",
        "    for s in ds:\n",
        "        if hasattr(s, \"eval_sample\"):\n",
        "            s.eval_sample.response = \"\"\n",
        "            s.eval_sample.retrieved_contexts = []\n",
        "\n",
        "def to_text(x):\n",
        "    return x.content if isinstance(x, AIMessage) else str(x)\n",
        "\n",
        "def make_chain(retriever, prompt, llm):\n",
        "    return (\n",
        "        {\"context\": itemgetter(\"question\") | retriever, \"question\": itemgetter(\"question\")}\n",
        "        | RunnablePassthrough.assign(context=itemgetter(\"context\"))\n",
        "        | {\"response\": prompt | llm, \"context\": itemgetter(\"context\")}\n",
        "    )\n",
        "\n",
        "def evaluate_current_dataset(ds, evaluator_llm):\n",
        "    evaluation_dataset = EvaluationDataset.from_pandas(ds.to_pandas())\n",
        "    return evaluate(\n",
        "        dataset=evaluation_dataset,\n",
        "        metrics=[\n",
        "            LLMContextRecall(), Faithfulness(), FactualCorrectness()\n",
        "        ],\n",
        "        llm=evaluator_llm,\n",
        "        run_config=RunConfig(timeout=360),\n",
        "        raise_exceptions=False,\n",
        "    )\n",
        "\n",
        "evaluator_llm = LangchainLLMWrapper(chat_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**NAIVE retriever evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "824311025b424e55b04010d1f93fdafb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Exception raised in Job[20]: PermissionDeniedError(Error code: 403)\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[34]: LLMDidNotFinishException(The LLM generation was not completed. Please increase the max_tokens and try again.)\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 1.0000, 'faithfulness': 0.9492, 'factual_correctness(mode=f1)': 0.6900, 'answer_relevancy': 0.8274, 'context_entity_recall': 0.2358, 'noise_sensitivity(mode=relevant)': 0.5278}\n"
          ]
        }
      ],
      "source": [
        "naive_chain = make_chain(pdf_naive_retriever, rag_prompt, chat_model)\n",
        "\n",
        "reset_eval_fields(dataset)\n",
        "for row in dataset:\n",
        "    q = getattr(row.eval_sample, \"user_input\", None) or getattr(row.eval_sample, \"question\", None)\n",
        "    if not q: \n",
        "        continue\n",
        "    out = naive_chain.invoke({\"question\": q})\n",
        "    row.eval_sample.response = to_text(out[\"response\"])\n",
        "    row.eval_sample.retrieved_contexts = [d.page_content for d in out[\"context\"]][:10]\n",
        "\n",
        "# 3) Evaluate\n",
        "res_naive = evaluate_current_dataset(dataset, evaluator_llm)\n",
        "print(res_naive)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**BM25 retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74c9a413807a425ab6195d78874a556a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "Prompt fix_output_format failed to parse output: The output parser failed to parse the output including retries.\n",
            "Prompt extract_entities_prompt failed to parse output: The output parser failed to parse the output including retries.\n",
            "Exception raised in Job[16]: RagasOutputParserException(The output parser failed to parse the output including retries.)\n",
            "Exception raised in Job[11]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 1.0000, 'faithfulness': 0.8382, 'factual_correctness(mode=f1)': 0.6700, 'answer_relevancy': 0.8212, 'context_entity_recall': 0.1084, 'noise_sensitivity(mode=relevant)': 0.2000}\n"
          ]
        }
      ],
      "source": [
        "bm25_chain = make_chain(pdf_bm25_retriever, rag_prompt, chat_model)\n",
        "\n",
        "reset_eval_fields(dataset)\n",
        "for row in dataset:\n",
        "    q = getattr(row.eval_sample, \"user_input\", None) or getattr(row.eval_sample, \"question\", None)\n",
        "    if not q: \n",
        "        continue\n",
        "    out = bm25_chain.invoke({\"question\": q})\n",
        "    row.eval_sample.response = to_text(out[\"response\"])\n",
        "    row.eval_sample.retrieved_contexts = [d.page_content for d in out[\"context\"]][:10]\n",
        "\n",
        "res_bm25 = evaluate_current_dataset(dataset, evaluator_llm)\n",
        "print(res_bm25)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Multi-Query retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cd1c9cdf7eb49fb84287e691362d2c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/48 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "LLM returned 1 generations instead of requested 3. Proceeding with 1 generations.\n",
            "Exception raised in Job[11]: TimeoutError()\n",
            "Exception raised in Job[5]: TimeoutError()\n",
            "Exception raised in Job[28]: TimeoutError()\n",
            "Exception raised in Job[29]: TimeoutError()\n",
            "Exception raised in Job[35]: TimeoutError()\n",
            "Exception raised in Job[41]: TimeoutError()\n",
            "Exception raised in Job[47]: TimeoutError()\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'context_recall': 1.0000, 'faithfulness': 0.9861, 'factual_correctness(mode=f1)': 0.6700, 'answer_relevancy': 0.9376, 'context_entity_recall': 0.3073, 'noise_sensitivity(mode=relevant)': 0.3500}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mq_chain = make_chain(pdf_multi_query_retriever, rag_prompt, chat_model)\n",
        "\n",
        "reset_eval_fields(dataset)\n",
        "for row in dataset:\n",
        "    q = getattr(row.eval_sample, \"user_input\", None) or getattr(row.eval_sample, \"question\", None)\n",
        "    if not q: \n",
        "        continue\n",
        "    out = mq_chain.invoke({\"question\": q})\n",
        "    row.eval_sample.response = to_text(out[\"response\"])\n",
        "    row.eval_sample.retrieved_contexts = [d.page_content for d in out[\"context\"]][:10]\n",
        "\n",
        "res_mq = evaluate_current_dataset(dataset, evaluator_llm)\n",
        "res_mq\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Parent-Document retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f5c647f0e8d42e7a99c461c13bace32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 1.0000, 'faithfulness': 0.8750, 'factual_correctness(mode=f1)': 0.7637}\n"
          ]
        }
      ],
      "source": [
        "parent_chain = make_chain(pdf_parent_document_retriever, rag_prompt, chat_model)\n",
        "\n",
        "reset_eval_fields(dataset)\n",
        "for row in dataset:\n",
        "    q = getattr(row.eval_sample, \"user_input\", None) or getattr(row.eval_sample, \"question\", None)\n",
        "    if not q: \n",
        "        continue\n",
        "    out = parent_chain.invoke({\"question\": q})\n",
        "    row.eval_sample.response = to_text(out[\"response\"])\n",
        "    row.eval_sample.retrieved_contexts = [d.page_content for d in out[\"context\"]][:10]\n",
        "\n",
        "res_parent = evaluate_current_dataset(dataset, evaluator_llm)\n",
        "print(res_parent)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Compression retriever (Cohere rerank)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a4a69793cce4445b02dd96a7fad3675",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 1.0000, 'faithfulness': 0.8353, 'factual_correctness(mode=f1)': 0.5763}\n"
          ]
        }
      ],
      "source": [
        "compression_chain = make_chain(pdf_compression_retriever, rag_prompt, chat_model)\n",
        "\n",
        "reset_eval_fields(dataset)\n",
        "for row in dataset:\n",
        "    q = getattr(row.eval_sample, \"user_input\", None) or getattr(row.eval_sample, \"question\", None)\n",
        "    if not q: \n",
        "        continue\n",
        "    out = compression_chain.invoke({\"question\": q})\n",
        "    row.eval_sample.response = to_text(out[\"response\"])\n",
        "    row.eval_sample.retrieved_contexts = [d.page_content for d in out[\"context\"]][:10]\n",
        "\n",
        "res_compression = evaluate_current_dataset(dataset, evaluator_llm)\n",
        "print(res_compression)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Ensemble retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db56a2197f5248c8bfbf44452efae696",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/24 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'context_recall': 1.0000, 'faithfulness': 1.0000, 'factual_correctness(mode=f1)': 0.7963}\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers import EnsembleRetriever\n",
        "\n",
        "retriever_list = [\n",
        "    pdf_bm25_retriever,\n",
        "    pdf_naive_retriever,\n",
        "    pdf_multi_query_retriever,\n",
        "    pdf_parent_document_retriever,\n",
        "    pdf_compression_retriever,  \n",
        "]\n",
        "\n",
        "equal_weighting = [1/len(retriever_list)] * len(retriever_list)\n",
        "\n",
        "pdf_ensemble_retriever_all = EnsembleRetriever(\n",
        "    retrievers=retriever_list,\n",
        "    weights=equal_weighting,\n",
        ")\n",
        "\n",
        "ensemble_chain = make_chain(pdf_ensemble_retriever_all, rag_prompt, chat_model)\n",
        "\n",
        "reset_eval_fields(dataset)\n",
        "for row in dataset:\n",
        "    q = getattr(row.eval_sample, \"user_input\", None) or getattr(row.eval_sample, \"question\", None)\n",
        "    if not q:\n",
        "        continue\n",
        "    out = ensemble_chain.invoke({\"question\": q})\n",
        "    row.eval_sample.response = to_text(out[\"response\"])\n",
        "    row.eval_sample.retrieved_contexts = [d.page_content for d in out[\"context\"]][:10]\n",
        "\n",
        "res_ensemble = evaluate_current_dataset(dataset, evaluator_llm)\n",
        "print(res_ensemble)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ACTIVITY 1: RETRIEVER COMPARISON RESULTS\n",
            "================================================================================\n",
            "  Retriever                           Context Recall                                                            Faithfulness                            Factual Correctness\n",
            " Parent-Doc [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                                [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] [1.0, 0.33, 0.57, 0.62, 1.0, 0.92, 0.93, 0.74]\n",
            "   Ensemble [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                                [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  [1.0, 0.2, 1.0, 0.91, 0.83, 0.85, 0.92, 0.66]\n",
            "       BM25 [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0] [1.0, 0.0, 1.0, 0.8666666666666667, 0.95, 0.8888888888888888, 1.0, 1.0] [1.0, 0.0, 0.57, 0.31, 0.87, 0.94, 0.86, 0.81]\n",
            "Compression [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]  [1.0, 0.0, 0.7777777777777778, 1.0, 0.9047619047619048, 1.0, 1.0, 1.0] [0.5, 0.0, 0.18, 0.73, 0.87, 0.71, 0.93, 0.69]\n",
            "Multi-Query [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                 [1.0, 1.0, 1.0, 1.0, 0.8888888888888888, 1.0, 1.0, 1.0] [0.33, 0.8, 0.5, 0.36, 0.78, 0.92, 0.88, 0.79]\n",
            "      Naive [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]                [0.8, 1.0, 1.0, 1.0, 0.96, 1.0, 0.8333333333333334, 1.0]   [0.33, 0.0, 1.0, nan, 0.9, 0.84, 0.92, 0.84]\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "results_data = {\n",
        "    \"Retriever\": [\"Naive\", \"BM25\", \"Multi-Query\", \"Parent-Doc\", \"Compression\", \"Ensemble\"],\n",
        "    \"Context Recall\": [\n",
        "        res_naive['context_recall'],\n",
        "        res_bm25['context_recall'],\n",
        "        res_mq['context_recall'],\n",
        "        res_parent['context_recall'],\n",
        "        res_compression['context_recall'],\n",
        "        res_ensemble['context_recall']\n",
        "    ],\n",
        "    \"Faithfulness\": [\n",
        "        res_naive['faithfulness'],\n",
        "        res_bm25['faithfulness'],\n",
        "        res_mq['faithfulness'],\n",
        "        res_parent['faithfulness'],\n",
        "        res_compression['faithfulness'],\n",
        "        res_ensemble['faithfulness']\n",
        "    ],\n",
        "    \"Factual Correctness\": [\n",
        "        res_naive['factual_correctness(mode=f1)'],\n",
        "        res_bm25['factual_correctness(mode=f1)'],\n",
        "        res_mq['factual_correctness(mode=f1)'],\n",
        "        res_parent['factual_correctness(mode=f1)'],\n",
        "        res_compression['factual_correctness(mode=f1)'],\n",
        "        res_ensemble['factual_correctness(mode=f1)']\n",
        "    ],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_data)\n",
        "\n",
        "# Sort by Factual Correctness\n",
        "results_df = results_df.sort_values('Factual Correctness', ascending=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ACTIVITY 1: RETRIEVER COMPARISON RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(results_df.to_string(index=False))\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ANALYSIS OF RETRIEVAL STRATEGIES**\n",
        "\n",
        "Based on evaluation across 6 retrieval methods on 10 synthetic questions, the Ensemble \n",
        "retriever achieved the highest performance with perfect faithfulness (1.00) and factual \n",
        "correctness (0.80), demonstrating that combining multiple retrieval strategies (BM25, \n",
        "Naive, Multi-Query, Parent-Doc, and Compression) effectively captures diverse relevant \n",
        "information. However, this comes at significant cost‚Äîthe Ensemble approach incurs \n",
        "cumulative API expenses and latency from all constituent retrievers plus Cohere reranking.\n",
        "\n",
        "For production use prioritizing cost-efficiency, the Parent-Document retriever offers \n",
        "the best balance, achieving strong factual correctness (0.76) and faithfulness (0.88) \n",
        "with only single embedding and LLM calls per query. BM25 provides the lowest cost option \n",
        "(zero embedding fees) but trails in accuracy (0.67 factual). The Compression retriever, \n",
        "despite adding Cohere reranking costs, surprisingly underperformed (0.58 factual), \n",
        "suggesting the reranking may have filtered out relevant context for this particular dataset.\n",
        "\n",
        "All retrievers achieved perfect context recall (1.00), indicating the primary differentiator \n",
        "is not retrieval coverage but rather how well each method ranks and presents context to \n",
        "the LLM. For this PDF dataset about AI usage, I recommend the Parent-Document retriever \n",
        "for balanced production use, or Ensemble when accuracy justifies the higher operational cost."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
